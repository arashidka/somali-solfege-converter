{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Somali Solfege Converter - Audio Processing\n",
    "\n",
    "## PRSE (Python Rapid-Systems Engine) Mode\n",
    "\n",
    "This notebook implements **Phase 1: Initialization** of the Somali Solfege Converter system.\n",
    "\n",
    "### Design Blueprint: Phase 1\n",
    "\n",
    "* **Architectural Choice:** Procedural (Linear pipeline is most efficient for this automation)\n",
    "* **Key Libraries:** `moviepy` (Video handling), `scipy` (I/O), `numpy` (DSP)\n",
    "* **Memory Strategy:** Immediate conversion to `float32` and deletion of video objects after audio extraction\n",
    "* **Target Sample Rate:** 22.05kHz for memory efficiency on 8GB RAM systems\n",
    "\n",
    "### Supported Formats\n",
    "\n",
    "* **Video:** .mp4, .mov, .avi, .mkv (audio will be extracted automatically)\n",
    "* **Audio:** .wav, .mp3, .flac, .ogg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Environment & Dependencies\n",
    "\n",
    "This cell ensures your virtual environment is ready and all required libraries are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# List of required libraries\n",
    "libraries = ['moviepy', 'numpy', 'scipy', 'matplotlib', 'librosa']\n",
    "\n",
    "def check_setup():\n",
    "    print(\"Checking environment dependencies...\")\n",
    "    for lib in libraries:\n",
    "        try:\n",
    "            __import__(lib)\n",
    "            print(f\"\u2705 {lib} is installed.\")\n",
    "        except ImportError:\n",
    "            print(f\"\u274c {lib} missing. Installing now...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", lib])\n",
    "\n",
    "check_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Audio Extraction & Pre-processing\n",
    "\n",
    "This cell detects if your input is a video and extracts the audio stream, or loads an audio file directly. \n",
    "It performs the **22.05kHz downsampling** for memory efficiency.\n",
    "\n",
    "### Features:\n",
    "* Automatic video-to-audio extraction\n",
    "* Stereo-to-mono conversion\n",
    "* Downsampling to 22.05kHz\n",
    "* Memory-efficient float32 normalization\n",
    "* Automatic cleanup of temporary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Import moviepy - compatible with both v1.x and v2.x\n",
    "try:\n",
    "    from moviepy import VideoFileClip\n",
    "except ImportError:\n",
    "    from moviepy.editor import VideoFileClip\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "def prepare_audio_input(file_path, target_sr=22050):\n",
    "    \"\"\"\n",
    "    Extracts audio from video if needed and loads it into memory.\n",
    "    Optimized for 8GB RAM using float32 and downsampling.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Input file not found: {file_path}\")\n",
    "    \n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    temp_audio = \"temp_extracted_audio.wav\"\n",
    "    \n",
    "    # Step 1: Video to Audio Extraction (If needed)\n",
    "    if ext in ['.mp4', '.mov', '.avi', '.mkv']:\n",
    "        print(f\"Video detected. Extracting audio from {file_path}...\")\n",
    "        try:\n",
    "            video = VideoFileClip(file_path)\n",
    "            if video.audio is None:\n",
    "                raise ValueError(f\"Video file has no audio track: {file_path}\")\n",
    "            video.audio.write_audiofile(temp_audio, fps=target_sr, verbose=False, logger=None)\n",
    "            video.close() # Close file handle immediately\n",
    "            load_path = temp_audio\n",
    "            del video\n",
    "        except Exception as e:\n",
    "            if os.path.exists(temp_audio):\n",
    "                os.remove(temp_audio)\n",
    "            raise ValueError(f\"Failed to extract audio from video: {e}\")\n",
    "    elif ext in ['.wav', '.mp3', '.flac', '.ogg']:\n",
    "        load_path = file_path\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {ext}. \"\n",
    "                        f\"Supported: .mp4, .mov, .avi, .mkv, .wav, .mp3, .flac, .ogg\")\n",
    "\n",
    "    # Step 2: Load and Downsample\n",
    "    print(f\"Loading and normalizing audio...\")\n",
    "    try:\n",
    "        sr, data = wavfile.read(load_path)\n",
    "    except Exception as e:\n",
    "        if os.path.exists(temp_audio) and ext in ['.mp4', '.mov', '.avi', '.mkv']:\n",
    "            os.remove(temp_audio)\n",
    "        raise ValueError(f\"Failed to load audio file: {e}\")\n",
    "    \n",
    "    # Convert to Mono if Stereo\n",
    "    if len(data.shape) > 1:\n",
    "        print(f\"Converting stereo to mono...\")\n",
    "        data = data.mean(axis=1)\n",
    "    \n",
    "    # Downsample logic (Simple decimation for speed)\n",
    "    if sr != target_sr:\n",
    "        print(f\"Resampling from {sr}Hz to {target_sr}Hz...\")\n",
    "        resample_factor = max(1, sr // target_sr)\n",
    "        data = data[::resample_factor]\n",
    "        sr = sr // resample_factor\n",
    "    \n",
    "    # Memory-safe conversion to float32\n",
    "    if data.dtype in [np.int16, np.int32]:\n",
    "        # Integer audio data - normalize by type max\n",
    "        info = np.iinfo(data.dtype)\n",
    "        samples = data.astype(np.float32) / max(abs(info.min), abs(info.max))\n",
    "    else:\n",
    "        # Already float - just convert and normalize\n",
    "        samples = data.astype(np.float32)\n",
    "        max_val = np.max(np.abs(samples))\n",
    "        if max_val > 0:\n",
    "            samples /= max_val\n",
    "    \n",
    "    # Cleanup\n",
    "    if os.path.exists(temp_audio) and ext in ['.mp4', '.mov', '.avi', '.mkv']:\n",
    "        os.remove(temp_audio)\n",
    "    \n",
    "    del data\n",
    "    gc.collect()\n",
    "    \n",
    "    duration = len(samples) / sr\n",
    "    print(f\"\u2705 Done. Loaded {duration:.2f}s of audio at {sr}Hz.\")\n",
    "    return samples, sr\n",
    "\n",
    "# --- TEST THE CELL ---\n",
    "# Uncomment the lines below and provide your input file path\n",
    "# INPUT_FILE = \"your_video_or_audio_here.mp4\" \n",
    "# samples, sr = prepare_audio_input(INPUT_FILE)\n",
    "print(\"Audio extraction function ready. Set INPUT_FILE and run to process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Visualize Audio Waveform (Optional)\n",
    "\n",
    "Once you have loaded audio, you can visualize it to verify the extraction worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_waveform(samples, sr, duration_limit=10.0):\n",
    "    \"\"\"\n",
    "    Plot the audio waveform.\n",
    "    \n",
    "    Args:\n",
    "        samples: Audio samples array\n",
    "        sr: Sample rate\n",
    "        duration_limit: Maximum duration to plot in seconds (default: 10s)\n",
    "    \"\"\"\n",
    "    # Limit the plot to avoid memory issues\n",
    "    max_samples = int(duration_limit * sr)\n",
    "    plot_samples = samples[:max_samples]\n",
    "    \n",
    "    time = np.arange(len(plot_samples)) / sr\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(time, plot_samples, linewidth=0.5)\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.title(f'Audio Waveform (first {duration_limit}s)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- VISUALIZE AUDIO ---\n",
    "# Uncomment to visualize after loading audio\n",
    "# plot_waveform(samples, sr)\n",
    "print(\"Waveform visualization function ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 2: Pitch Detection & Note Segmentation\n",
    "\n",
    "This phase implements the **YIN Algorithm** for accurate pitch detection and note segmentation.\n",
    "\n",
    "### Design Blueprint: Phase 2\n",
    "\n",
    "* **Algorithm:** YIN (Yet another INtonation estimator) - robust for monophonic pitch tracking\n",
    "* **Frequency Range:** 80-800 Hz (suitable for voice and most melodic instruments)\n",
    "* **Post-processing:** Median filtering for smooth pitch tracks\n",
    "* **Note Segmentation:** Groups consecutive frames with similar pitch into notes\n",
    "* **Optimization:** Focused on clean frequency tracking for Somali Pentatonic practice\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "* **Frame Length:** 2048 samples (~93ms at 22.05kHz)\n",
    "* **Hop Length:** 512 samples (~23ms at 22.05kHz)\n",
    "* **Threshold:** 0.1 (lower = more sensitive, higher = more selective)\n",
    "* **Min Note Duration:** 0.1s (filters out very short notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import medfilt\n",
    "\n",
    "def yin_pitch_detection(audio_samples, sr, frame_length=2048, hop_length=512, \n",
    "                        threshold=0.1, freq_min=80, freq_max=800):\n",
    "    \"\"\"\n",
    "    YIN algorithm for robust pitch detection.\n",
    "    Optimized for clean frequency tracking in musical contexts.\n",
    "    \"\"\"\n",
    "    # Calculate lag range based on frequency limits\n",
    "    lag_min = int(sr / freq_max)\n",
    "    lag_max = int(sr / freq_min)\n",
    "    \n",
    "    # Number of frames\n",
    "    n_frames = 1 + (len(audio_samples) - frame_length) // hop_length\n",
    "    \n",
    "    # Output arrays\n",
    "    times = np.arange(n_frames) * hop_length / sr\n",
    "    pitches = np.zeros(n_frames)\n",
    "    \n",
    "    # Process each frame\n",
    "    for frame_idx in range(n_frames):\n",
    "        start = frame_idx * hop_length\n",
    "        end = start + frame_length\n",
    "        \n",
    "        if end > len(audio_samples):\n",
    "            break\n",
    "            \n",
    "        frame = audio_samples[start:end]\n",
    "        \n",
    "        # Step 1: Difference function\n",
    "        diff = np.zeros(lag_max)\n",
    "        for lag in range(lag_min, lag_max):\n",
    "            for j in range(frame_length - lag):\n",
    "                delta = frame[j] - frame[j + lag]\n",
    "                diff[lag] += delta * delta\n",
    "        \n",
    "        # Step 2: Cumulative mean normalized difference\n",
    "        cmnd = np.ones(lag_max)\n",
    "        cumsum = 0\n",
    "        for lag in range(lag_min, lag_max):\n",
    "            cumsum += diff[lag]\n",
    "            if cumsum > 0:\n",
    "                cmnd[lag] = diff[lag] / (cumsum / lag)\n",
    "        \n",
    "        # Step 3: Find first lag below threshold\n",
    "        pitch_lag = None\n",
    "        for lag in range(lag_min, lag_max):\n",
    "            if cmnd[lag] < threshold:\n",
    "                pitch_lag = lag\n",
    "                break\n",
    "        \n",
    "        # Step 4: Parabolic interpolation for accuracy\n",
    "        if pitch_lag is not None and lag_min < pitch_lag < lag_max - 1:\n",
    "            alpha = cmnd[pitch_lag - 1]\n",
    "            beta = cmnd[pitch_lag]\n",
    "            gamma = cmnd[pitch_lag + 1]\n",
    "            \n",
    "            if alpha > beta and gamma > beta:\n",
    "                peak_offset = 0.5 * (alpha - gamma) / (alpha - 2 * beta + gamma)\n",
    "                refined_lag = pitch_lag + peak_offset\n",
    "                pitches[frame_idx] = sr / refined_lag\n",
    "            else:\n",
    "                pitches[frame_idx] = sr / pitch_lag\n",
    "        else:\n",
    "            pitches[frame_idx] = 0.0\n",
    "    \n",
    "    return times, pitches\n",
    "\n",
    "# --- DETECT PITCH ---\n",
    "# Uncomment after loading audio with prepare_audio_input()\n",
    "# times, raw_pitches = yin_pitch_detection(samples, sr)\n",
    "# print(f\"Detected {len(times)} frames over {times[-1]:.2f}s\")\n",
    "# print(f\"Pitch range: {raw_pitches[raw_pitches > 0].min():.1f} - {raw_pitches[raw_pitches > 0].max():.1f} Hz\")\n",
    "print(\"YIN pitch detection function ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Pitch Smoothing & Note Segmentation\n",
    "\n",
    "This cell smooths the raw pitch track and segments it into individual musical notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_pitch_track(pitches, kernel_size=5):\n",
    "    \"\"\"\n",
    "    Smooth the pitch track using median filtering.\n",
    "    Removes outliers while preserving note transitions.\n",
    "    \"\"\"\n",
    "    smoothed = pitches.copy()\n",
    "    non_zero_mask = pitches > 0\n",
    "    \n",
    "    if np.sum(non_zero_mask) > kernel_size:\n",
    "        smoothed[non_zero_mask] = medfilt(pitches[non_zero_mask], kernel_size=kernel_size)\n",
    "    \n",
    "    return smoothed\n",
    "\n",
    "def segment_notes(times, pitches, min_note_duration=0.1, pitch_tolerance=20):\n",
    "    \"\"\"\n",
    "    Segment the pitch track into individual notes.\n",
    "    Groups consecutive frames with similar pitch.\n",
    "    \"\"\"\n",
    "    if len(times) == 0 or len(pitches) == 0:\n",
    "        return []\n",
    "    \n",
    "    notes = []\n",
    "    current_note = None\n",
    "    \n",
    "    for i, (time, pitch) in enumerate(zip(times, pitches)):\n",
    "        if pitch > 0:  # Voiced frame\n",
    "            if current_note is None:\n",
    "                current_note = {\n",
    "                    'start_time': time,\n",
    "                    'start_idx': i,\n",
    "                    'pitches': [pitch]\n",
    "                }\n",
    "            else:\n",
    "                mean_pitch = np.mean(current_note['pitches'])\n",
    "                if abs(pitch - mean_pitch) < pitch_tolerance:\n",
    "                    current_note['pitches'].append(pitch)\n",
    "                else:\n",
    "                    # End current note\n",
    "                    end_time = times[i - 1] if i > 0 else time\n",
    "                    duration = end_time - current_note['start_time']\n",
    "                    \n",
    "                    if duration >= min_note_duration:\n",
    "                        current_note['end_time'] = end_time\n",
    "                        current_note['duration'] = duration\n",
    "                        current_note['mean_pitch'] = np.mean(current_note['pitches'])\n",
    "                        current_note['median_pitch'] = np.median(current_note['pitches'])\n",
    "                        notes.append(current_note)\n",
    "                    \n",
    "                    # Start new note\n",
    "                    current_note = {\n",
    "                        'start_time': time,\n",
    "                        'start_idx': i,\n",
    "                        'pitches': [pitch]\n",
    "                    }\n",
    "        else:  # Unvoiced frame\n",
    "            if current_note is not None:\n",
    "                end_time = times[i - 1] if i > 0 else time\n",
    "                duration = end_time - current_note['start_time']\n",
    "                \n",
    "                if duration >= min_note_duration:\n",
    "                    current_note['end_time'] = end_time\n",
    "                    current_note['duration'] = duration\n",
    "                    current_note['mean_pitch'] = np.mean(current_note['pitches'])\n",
    "                    current_note['median_pitch'] = np.median(current_note['pitches'])\n",
    "                    notes.append(current_note)\n",
    "                \n",
    "                current_note = None\n",
    "    \n",
    "    # Handle last note\n",
    "    if current_note is not None:\n",
    "        end_time = times[-1]\n",
    "        duration = end_time - current_note['start_time']\n",
    "        \n",
    "        if duration >= min_note_duration:\n",
    "            current_note['end_time'] = end_time\n",
    "            current_note['duration'] = duration\n",
    "            current_note['mean_pitch'] = np.mean(current_note['pitches'])\n",
    "            current_note['median_pitch'] = np.median(current_note['pitches'])\n",
    "            notes.append(current_note)\n",
    "    \n",
    "    return notes\n",
    "\n",
    "# --- PROCESS PITCH TRACK ---\n",
    "# Uncomment after detecting pitch\n",
    "# smoothed_pitches = smooth_pitch_track(raw_pitches, kernel_size=5)\n",
    "# detected_notes = segment_notes(times, smoothed_pitches, min_note_duration=0.1, pitch_tolerance=20)\n",
    "# print(f\"Detected {len(detected_notes)} notes\")\n",
    "# for i, note in enumerate(detected_notes[:5]):\n",
    "#     print(f\"Note {i+1}: {note['mean_pitch']:.1f} Hz, duration: {note['duration']:.2f}s\")\n",
    "print(\"Pitch smoothing and note segmentation functions ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Visualize Pitch Detection Results\n",
    "\n",
    "Visualize the pitch track and detected notes to verify the detection quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pitch_track(times, raw_pitches, smoothed_pitches=None, notes=None, duration_limit=10.0):\n",
    "    \"\"\"\n",
    "    Plot the pitch track with optional smoothing and note boundaries.\n",
    "    \"\"\"\n",
    "    # Limit to duration\n",
    "    mask = times <= duration_limit\n",
    "    times_plot = times[mask]\n",
    "    raw_plot = raw_pitches[mask]\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Plot raw pitch track\n",
    "    voiced_mask = raw_plot > 0\n",
    "    plt.plot(times_plot[voiced_mask], raw_plot[voiced_mask], 'o', \n",
    "             alpha=0.3, markersize=2, label='Raw pitch', color='gray')\n",
    "    \n",
    "    # Plot smoothed pitch track if provided\n",
    "    if smoothed_pitches is not None:\n",
    "        smoothed_plot = smoothed_pitches[mask]\n",
    "        voiced_mask_smooth = smoothed_plot > 0\n",
    "        plt.plot(times_plot[voiced_mask_smooth], smoothed_plot[voiced_mask_smooth], \n",
    "                 '-', linewidth=2, label='Smoothed pitch', color='blue')\n",
    "    \n",
    "    # Plot note boundaries if provided\n",
    "    if notes is not None:\n",
    "        for note in notes:\n",
    "            if note['start_time'] <= duration_limit:\n",
    "                plt.axvline(note['start_time'], color='green', linestyle='--', alpha=0.5)\n",
    "                plt.axhline(note['mean_pitch'], \n",
    "                           xmin=note['start_time']/duration_limit, \n",
    "                           xmax=min(note['end_time'], duration_limit)/duration_limit,\n",
    "                           color='red', linewidth=3, alpha=0.6)\n",
    "    \n",
    "    plt.xlabel('Time (seconds)', fontsize=12)\n",
    "    plt.ylabel('Frequency (Hz)', fontsize=12)\n",
    "    plt.title('Pitch Detection Results', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- VISUALIZE PITCH ---\n",
    "# Uncomment after pitch detection\n",
    "# plot_pitch_track(times, raw_pitches, smoothed_pitches, detected_notes, duration_limit=10.0)\n",
    "print(\"Pitch visualization function ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Complete Workflow Example\n",
    "\n",
    "Here's how to use all the cells together:\n",
    "\n",
    "```python\n",
    "# Step 1: Load audio (Cell 2)\n",
    "INPUT_FILE = \"your_audio.mp4\"\n",
    "samples, sr = prepare_audio_input(INPUT_FILE, target_sr=22050)\n",
    "\n",
    "# Step 2: Detect pitch (Cell 4)\n",
    "times, raw_pitches = yin_pitch_detection(samples, sr)\n",
    "\n",
    "# Step 3: Smooth and segment (Cell 5)\n",
    "smoothed_pitches = smooth_pitch_track(raw_pitches, kernel_size=5)\n",
    "detected_notes = segment_notes(times, smoothed_pitches, \n",
    "                               min_note_duration=0.1, \n",
    "                               pitch_tolerance=20)\n",
    "\n",
    "# Step 4: Visualize (Cell 6)\n",
    "plot_pitch_track(times, raw_pitches, smoothed_pitches, detected_notes)\n",
    "\n",
    "# Step 5: Analyze results\n",
    "print(f\"Total notes detected: {len(detected_notes)}\")\n",
    "for i, note in enumerate(detected_notes):\n",
    "    print(f\"Note {i+1}: {note['mean_pitch']:.1f} Hz, \"\n",
    "          f\"duration: {note['duration']:.2f}s, \"\n",
    "          f\"time: {note['start_time']:.2f}-{note['end_time']:.2f}s\")\n",
    "```\n",
    "\n",
    "## Next Steps: Phase 3 - Somali Pentatonic Scale Mapping\n",
    "\n",
    "The next phase will implement:\n",
    "1. **Pentatonic Scale Detection**: Identify the tonic and scale type\n",
    "2. **Solfege Mapping**: Map detected notes to Somali solfege syllables\n",
    "3. **Export Functionality**: Save results in various formats (JSON, MIDI, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}